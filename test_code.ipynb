{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6d75766",
   "metadata": {},
   "source": [
    "Importing all needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e5055aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\esian\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import Dense\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd9fc2e",
   "metadata": {},
   "source": [
    "Testing NN ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe96f936",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-10 19:55:19,338] A new study created in memory with name: no-name-431a4bcc-2ddf-41c8-a9a3-ef7860d5bb60\n",
      "[I 2025-09-10 19:55:30,776] Trial 0 finished with value: 4.914270877838135 and parameters: {'n_units': 141, 'n_layers': 3, 'dropout': 0.3464972793912885, 'learning_rate': 0.003629910797814957}. Best is trial 0 with value: 4.914270877838135.\n",
      "[I 2025-09-10 19:55:39,291] Trial 1 finished with value: 4.915328025817871 and parameters: {'n_units': 41, 'n_layers': 2, 'dropout': 0.35968768274406915, 'learning_rate': 0.0008820742278926103}. Best is trial 0 with value: 4.914270877838135.\n",
      "[I 2025-09-10 19:55:45,589] Trial 2 finished with value: 8.525769233703613 and parameters: {'n_units': 131, 'n_layers': 1, 'dropout': 0.14082252733475614, 'learning_rate': 0.0012689123440447856}. Best is trial 0 with value: 4.914270877838135.\n",
      "[I 2025-09-10 19:55:57,434] Trial 3 finished with value: 4.9405341148376465 and parameters: {'n_units': 159, 'n_layers': 2, 'dropout': 0.1454812963931277, 'learning_rate': 0.001437017137416862}. Best is trial 0 with value: 4.914270877838135.\n",
      "[I 2025-09-10 19:56:07,963] Trial 4 finished with value: 4.90887451171875 and parameters: {'n_units': 219, 'n_layers': 3, 'dropout': 0.3101682979397783, 'learning_rate': 0.004130741115523244}. Best is trial 4 with value: 4.90887451171875.\n",
      "[I 2025-09-10 19:56:15,730] Trial 5 finished with value: 4.923642635345459 and parameters: {'n_units': 63, 'n_layers': 2, 'dropout': 0.21809810579618782, 'learning_rate': 0.0017216159824882584}. Best is trial 4 with value: 4.90887451171875.\n",
      "[I 2025-09-10 19:56:25,780] Trial 6 finished with value: 4.912332534790039 and parameters: {'n_units': 256, 'n_layers': 3, 'dropout': 0.34834773797792284, 'learning_rate': 0.0003850321535679488}. Best is trial 4 with value: 4.90887451171875.\n",
      "[I 2025-09-10 19:56:34,767] Trial 7 finished with value: 4.918325901031494 and parameters: {'n_units': 125, 'n_layers': 3, 'dropout': 0.16527019070967378, 'learning_rate': 0.000290182899536556}. Best is trial 4 with value: 4.90887451171875.\n",
      "[I 2025-09-10 19:56:42,346] Trial 8 finished with value: 4.907493591308594 and parameters: {'n_units': 62, 'n_layers': 2, 'dropout': 0.20559286451377276, 'learning_rate': 0.0062183880717550325}. Best is trial 8 with value: 4.907493591308594.\n",
      "[I 2025-09-10 19:56:49,419] Trial 9 finished with value: 7.486470699310303 and parameters: {'n_units': 208, 'n_layers': 1, 'dropout': 0.2589367302321317, 'learning_rate': 0.0028359013463700082}. Best is trial 8 with value: 4.907493591308594.\n",
      "[I 2025-09-10 19:56:57,323] Trial 10 finished with value: 4.904933929443359 and parameters: {'n_units': 72, 'n_layers': 2, 'dropout': 0.47707782483175504, 'learning_rate': 0.009596113234873882}. Best is trial 10 with value: 4.904933929443359.\n",
      "[I 2025-09-10 19:57:05,000] Trial 11 finished with value: 4.931976795196533 and parameters: {'n_units': 83, 'n_layers': 2, 'dropout': 0.47440336292133956, 'learning_rate': 0.00987176131529003}. Best is trial 10 with value: 4.904933929443359.\n",
      "[I 2025-09-10 19:57:12,855] Trial 12 finished with value: 4.901871681213379 and parameters: {'n_units': 21, 'n_layers': 2, 'dropout': 0.4744782082801334, 'learning_rate': 0.009490461859535842}. Best is trial 12 with value: 4.901871681213379.\n",
      "[I 2025-09-10 19:57:20,063] Trial 13 finished with value: 5.209217071533203 and parameters: {'n_units': 21, 'n_layers': 1, 'dropout': 0.49679951652712884, 'learning_rate': 0.009646625946390194}. Best is trial 12 with value: 4.901871681213379.\n",
      "[I 2025-09-10 19:57:27,963] Trial 14 finished with value: 4.917351245880127 and parameters: {'n_units': 101, 'n_layers': 2, 'dropout': 0.42713969688235043, 'learning_rate': 0.0005933554114347387}. Best is trial 12 with value: 4.901871681213379.\n",
      "[I 2025-09-10 19:57:35,008] Trial 15 finished with value: 6.021068096160889 and parameters: {'n_units': 31, 'n_layers': 1, 'dropout': 0.4254710770062756, 'learning_rate': 0.006028822132260376}. Best is trial 12 with value: 4.901871681213379.\n",
      "[I 2025-09-10 19:57:43,550] Trial 16 finished with value: 4.924074649810791 and parameters: {'n_units': 93, 'n_layers': 2, 'dropout': 0.4248162776467739, 'learning_rate': 0.002007677472506928}. Best is trial 12 with value: 4.901871681213379.\n",
      "[I 2025-09-10 19:57:50,808] Trial 17 finished with value: 4.922952175140381 and parameters: {'n_units': 54, 'n_layers': 3, 'dropout': 0.4503277326442372, 'learning_rate': 0.00558859048557471}. Best is trial 12 with value: 4.901871681213379.\n",
      "[I 2025-09-10 19:57:58,504] Trial 18 finished with value: 9.356424331665039 and parameters: {'n_units': 79, 'n_layers': 1, 'dropout': 0.3889140463245546, 'learning_rate': 0.0001795968293718629}. Best is trial 12 with value: 4.901871681213379.\n",
      "[I 2025-09-10 19:58:09,461] Trial 19 finished with value: 9.02177619934082 and parameters: {'n_units': 20, 'n_layers': 2, 'dropout': 0.465000618360097, 'learning_rate': 0.00012861519918780885}. Best is trial 12 with value: 4.901871681213379.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'n_units': 21, 'n_layers': 2, 'dropout': 0.4744782082801334, 'learning_rate': 0.009490461859535842}\n",
      "Epoch 1/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 104.7632 - mae: 8.2037 - val_loss: 103.9238 - val_mae: 8.1851\n",
      "Epoch 2/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 102.8513 - mae: 8.0899 - val_loss: 103.8718 - val_mae: 8.1820\n",
      "Epoch 3/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 102.7980 - mae: 8.0874 - val_loss: 103.8634 - val_mae: 8.1815\n",
      "Epoch 4/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 102.7846 - mae: 8.0863 - val_loss: 103.8610 - val_mae: 8.1813\n",
      "Epoch 5/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 102.7732 - mae: 8.0857 - val_loss: 103.8601 - val_mae: 8.1813\n",
      "Epoch 6/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 102.7690 - mae: 8.0854 - val_loss: 103.8598 - val_mae: 8.1813\n",
      "Epoch 7/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 102.7680 - mae: 8.0852 - val_loss: 103.8596 - val_mae: 8.1813\n",
      "Epoch 8/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 102.7605 - mae: 8.0849 - val_loss: 103.8595 - val_mae: 8.1813\n",
      "Epoch 9/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 102.7572 - mae: 8.0848 - val_loss: 103.8595 - val_mae: 8.1813\n",
      "Epoch 10/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 102.7576 - mae: 8.0848 - val_loss: 103.8594 - val_mae: 8.1813\n",
      "Epoch 11/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 102.7606 - mae: 8.0848 - val_loss: 103.8594 - val_mae: 8.1813\n",
      "Epoch 12/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 102.7584 - mae: 8.0847 - val_loss: 103.8594 - val_mae: 8.1813\n",
      "Epoch 13/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 102.7544 - mae: 8.0846 - val_loss: 103.8594 - val_mae: 8.1813\n",
      "Epoch 14/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 102.7545 - mae: 8.0846 - val_loss: 103.8594 - val_mae: 8.1812\n",
      "Epoch 15/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 102.7562 - mae: 8.0847 - val_loss: 103.8594 - val_mae: 8.1812\n",
      "Epoch 16/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 102.7524 - mae: 8.0844 - val_loss: 103.8594 - val_mae: 8.1812\n",
      "Epoch 17/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 102.7557 - mae: 8.0846 - val_loss: 103.8594 - val_mae: 8.1812\n",
      "Epoch 18/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 102.7534 - mae: 8.0845 - val_loss: 103.8594 - val_mae: 8.1812\n",
      "Epoch 19/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 102.7523 - mae: 8.0844 - val_loss: 103.8594 - val_mae: 8.1812\n",
      "Epoch 20/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 102.7516 - mae: 8.0844 - val_loss: 103.8594 - val_mae: 8.1812\n",
      "Epoch 21/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 102.7511 - mae: 8.0844 - val_loss: 103.8594 - val_mae: 8.1812\n",
      "Epoch 22/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 102.7523 - mae: 8.0844 - val_loss: 103.8594 - val_mae: 8.1812\n",
      "Epoch 23/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 102.7516 - mae: 8.0844 - val_loss: 103.8594 - val_mae: 8.1812\n",
      "Epoch 24/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 102.7507 - mae: 8.0843 - val_loss: 103.8594 - val_mae: 8.1812\n",
      "Epoch 25/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 102.7497 - mae: 8.0843 - val_loss: 103.8594 - val_mae: 8.1812\n",
      "Epoch 26/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 102.7554 - mae: 8.0847 - val_loss: 103.8594 - val_mae: 8.1812\n",
      "Epoch 27/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 102.7623 - mae: 8.0850 - val_loss: 103.8594 - val_mae: 8.1812\n",
      "Epoch 28/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 102.7568 - mae: 8.0845 - val_loss: 103.8594 - val_mae: 8.1812\n",
      "Epoch 29/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 102.7496 - mae: 8.0843 - val_loss: 103.8594 - val_mae: 8.1812\n",
      "Epoch 30/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 102.7509 - mae: 8.0843 - val_loss: 103.8594 - val_mae: 8.1812\n",
      "Epoch 31/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 102.7499 - mae: 8.0843 - val_loss: 103.8594 - val_mae: 8.1812\n",
      "Epoch 32/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 102.7497 - mae: 8.0843 - val_loss: 103.8594 - val_mae: 8.1812\n",
      "Epoch 33/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 102.7515 - mae: 8.0845 - val_loss: 103.8594 - val_mae: 8.1812\n",
      "Epoch 34/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 102.7503 - mae: 8.0843 - val_loss: 103.8594 - val_mae: 8.1812\n",
      "Epoch 35/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 102.7497 - mae: 8.0843 - val_loss: 103.8594 - val_mae: 8.1812\n",
      "Epoch 36/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 102.7493 - mae: 8.0842 - val_loss: 103.8594 - val_mae: 8.1812\n",
      "Epoch 37/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 102.7500 - mae: 8.0843 - val_loss: 103.8594 - val_mae: 8.1812\n",
      "Epoch 38/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 102.7496 - mae: 8.0842 - val_loss: 103.8594 - val_mae: 8.1812\n",
      "Epoch 39/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 102.7533 - mae: 8.0844 - val_loss: 103.8594 - val_mae: 8.1812\n",
      "Epoch 40/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 102.7505 - mae: 8.0843 - val_loss: 103.8594 - val_mae: 8.1812\n",
      "Epoch 41/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 102.7514 - mae: 8.0844 - val_loss: 103.8594 - val_mae: 8.1812\n",
      "Epoch 42/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 102.7500 - mae: 8.0843 - val_loss: 103.8594 - val_mae: 8.1812\n",
      "Epoch 43/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 102.7497 - mae: 8.0843 - val_loss: 103.8594 - val_mae: 8.1812\n",
      "Epoch 44/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 102.7494 - mae: 8.0842 - val_loss: 103.8594 - val_mae: 8.1812\n",
      "Epoch 45/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 102.7552 - mae: 8.0844 - val_loss: 103.8594 - val_mae: 8.1812\n",
      "Epoch 46/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 102.7495 - mae: 8.0843 - val_loss: 103.8594 - val_mae: 8.1812\n",
      "Epoch 47/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 102.7496 - mae: 8.0842 - val_loss: 103.8594 - val_mae: 8.1812\n",
      "Epoch 48/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 102.7498 - mae: 8.0843 - val_loss: 103.8594 - val_mae: 8.1812\n",
      "Epoch 49/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 102.7495 - mae: 8.0842 - val_loss: 103.8594 - val_mae: 8.1812\n",
      "Epoch 50/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 102.7507 - mae: 8.0844 - val_loss: 103.8594 - val_mae: 8.1812\n",
      "Final Test MAE: 8.181249618530273, 103.859375\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"C:\\Users\\esian\\Desktop\\Kafe\\src\\model\\merged_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "encoder = LabelEncoder()\n",
    "df['item_encoded'] = encoder.fit_transform(df['item'])\n",
    "\n",
    "X = df[['day_of_week', 'is_holiday', 'temperature_2m_max', \n",
    "             'temperature_2m_min', 'precipitation_sum', 'item_encoded']]  \n",
    "y = df['sales'].fillna(0)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, train_size=0.8, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Optuna objective function\n",
    "# -----------------------------\n",
    "def objective(trial):\n",
    "    # Hyperparameters to search\n",
    "    n_units = trial.suggest_int(\"n_units\", 16, 256)\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "\n",
    "    # Build model\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(X_train.shape[1],)))\n",
    "\n",
    "    for i in range(n_layers):\n",
    "        model.add(layers.Dense(n_units, activation=\"relu\"))\n",
    "        model.add(layers.Dropout(dropout))\n",
    "\n",
    "    model.add(layers.Dense(1, activation=\"linear\"))  # regression output\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=\"mse\",\n",
    "        metrics=[\"mae\"]\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=20,  # keep small for tuning\n",
    "        batch_size=32,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Return the last validation MAE\n",
    "    val_mae = history.history[\"val_mae\"][-1]\n",
    "    return val_mae\n",
    "\n",
    "# -----------------------------\n",
    "# Run Optuna optimization\n",
    "# -----------------------------\n",
    "study = optuna.create_study(direction=\"minimize\")  # minimize MAE\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"Best params:\", study.best_params)\n",
    "\n",
    "# -----------------------------\n",
    "# Train final model with best params\n",
    "# -----------------------------\n",
    "best_params = study.best_params\n",
    "final_model = keras.Sequential()\n",
    "final_model.add(layers.Input(shape=(X_train.shape[1],)))\n",
    "\n",
    "for i in range(best_params[\"n_layers\"]):\n",
    "    final_model.add(layers.Dense(best_params[\"n_units\"], activation=\"relu\"))\n",
    "    final_model.add(layers.Dropout(best_params[\"dropout\"]))\n",
    "\n",
    "final_model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "final_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=best_params[\"learning_rate\"]),\n",
    "    loss=\"mse\",\n",
    "    metrics=[\"mae\"]\n",
    ")\n",
    "\n",
    "final_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=16)\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_mae = final_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Final Test MAE: {test_mae}, {test_loss}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc9e8e5",
   "metadata": {},
   "source": [
    "Testing Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b20da866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1997, 10) (1997,)\n",
      "Mean Squared Error: 23.656068021944442\n",
      "Mean Absolute Error: 3.8844558333333334\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import warnings\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# open sample data fiel\n",
    "file_path = (r\"C:\\Users\\esian\\Desktop\\Kafe\\src\\model\\merged_data.csv\")\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Encode item column\n",
    "encoder = LabelEncoder()\n",
    "df['item_encoded'] = encoder.fit_transform(df['item'])\n",
    "\n",
    "# set features\n",
    "X = df[['day_of_week', 'is_holiday','sales_lag_1', 'sales_lag_2', 'sales_lag_3', \n",
    "        'sales_lag_7','temperature_2m_max', 'temperature_2m_min', 'precipitation_sum','item_encoded']]  \n",
    "\n",
    "# check shape\n",
    "print(X.shape, y.shape) \n",
    "y = y.fillna(0) \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42, shuffle=True)\n",
    "\n",
    "# Create Random Forest regressor\n",
    "regressor = RandomForestRegressor(n_estimators=100, random_state=42, oob_score=True)\n",
    "\n",
    "# Train the model\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error:\", mse)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error:\", mae)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
